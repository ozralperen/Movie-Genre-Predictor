{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ozral\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ozral\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json # to open json files which contain the movie infos\n",
    "import os # to find the files that are going to be opened\n",
    "import pandas as pd # to do dataframe operations\n",
    "\n",
    "import nltk # to do word preprocessing\n",
    "from nltk.corpus import stopwords # to remove stopwords\n",
    "from nltk.tokenize import word_tokenize # to tokenize words\n",
    "from nltk.stem import WordNetLemmatizer #to lemmatize words\n",
    "import string # to do all other string operations\n",
    "\n",
    "import numpy as np # import numpy to do matrix operations with integers faster\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # to tokenize text to later use as neural network inputs\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #used for padding squences rapidly\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from time import time\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D, Convolution1D, LSTM, Conv2D, MaxPooling1D \n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import layers\n",
    "#used for creating and training neural networks\n",
    "\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score # to calculate different success rates\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def find_content(line, which_quote): #used for finding movie genres and synopsis in files\n",
    "    j = 0\n",
    "    k = -1\n",
    "    while (line[j] != '\"' or which_quote != 1):\n",
    "        if (line[j] == '\"'):\n",
    "            which_quote-=1\n",
    "        j+=1\n",
    "    while (line[k] != '\"'):\n",
    "        k-=1\n",
    "    return line[j+1:k]\n",
    "        \n",
    "\n",
    "def add_to_dataframe(json_path): # used for adding data in each file to a dataframe\n",
    "    descriptions = [] #array of descriptions\n",
    "    genres = [] #array of genres\n",
    "    #with open(\"data_tekli/\" + json_path, \"r\") as movie_infos: #opens given file\n",
    "    with open(\"data/\" + json_path, \"r\") as movie_infos: #opens given file\n",
    "        data = json.load(movie_infos) # loads the data in the file\n",
    "        for p in data['data']: # for each movie info in the file\n",
    "            appending = [] # next piece of data to be added to the dataframe\n",
    "            if \"overview\" in p and \"genres\" in p: # if overview and genres are not empty\n",
    "                for x in p.get(\"genres\"): #get genres of the next movie\n",
    "                    appending.append(x) # append these genres\n",
    "                genres.append(appending) # append newly added genres as an array\n",
    "                descriptions.append(p.get(\"overview\")) # get synopsis of the next movie\n",
    "        print(\"reading \" + json_path)\n",
    "    lemmatizer = WordNetLemmatizer() # new lemmatizer object\n",
    "    stop_words = set(stopwords.words('english')) #denotes that English stopwords will be removed\n",
    "    eng_words = set(nltk.corpus.words.words()) # list of English words\n",
    "    table = str.maketrans(\"\",\"\",string.punctuation) #table that will help to remove punctuation marks\n",
    "    for i in range(len(descriptions)): # for each description\n",
    "        word_tokens = word_tokenize(str(descriptions[i])) #tokenize description\n",
    "        filtered_sentence = [lemmatizer.lemmatize(w.lower()) for w in word_tokens if (not lemmatizer.lemmatize(w.lower()) in stop_words) and (w.isalpha()) and (lemmatizer.lemmatize(w.lower()) in eng_words)] #remove the stop words, get the words that exist in the English language, lower them and save them to an array\n",
    "        descriptions[i] = word_tokenize(str(filtered_sentence).translate(table))#string is cleansed from punctuation marks\n",
    "        \n",
    "    df2 = pd.DataFrame(zip(descriptions, genres), columns = ['descs', 'genres']) # create dataframe from given array of information with tags of descs and genres\n",
    "    return df2 #returns the new dataframe\n",
    "\n",
    "def pad_text(encoded_syn, maxlen): # function used for padding a tokenized words array\n",
    "    while len(encoded_syn) < maxlen: # while a given array is shorter than a length\n",
    "        encoded_syn.append(0) # add 0s to the end of the array\n",
    "    return encoded_syn # returns padded array\n",
    "\n",
    "def remove_spaces(x): # removes spaces between words\n",
    "    nospace=[]\n",
    "    for item in x:\n",
    "        item=item.lstrip()\n",
    "        nospace.append(item)\n",
    "    return (\",\").join(nospace)\n",
    "\n",
    "def genre_vectorize(genre_to_be_vectorized): # vectorizes genres of each movie with 0s and 1s\n",
    "    final = [] # final array to be used\n",
    "    for i in range(len(genre_to_be_vectorized)): # for each genre\n",
    "        cur_genres = genre_to_be_vectorized.iloc[i].split(\",\") # seperate genres in each movie\n",
    "        array_to_be_added = [] \n",
    "        for genre in genres_array: # make an array that will denote each movie genre with 0s and 1s\n",
    "            if genre in cur_genres:\n",
    "                array_to_be_added.append(1)\n",
    "            else:\n",
    "                array_to_be_added.append(0)\n",
    "        final.append(array_to_be_added) # makes all vectorized arrays into a matrix to train/test them in neural networks\n",
    "    return np.array(final, dtype = np.int64) # return the matrix as a numpy array\n",
    "\n",
    "def f1micro(y_true, y_pred): # used for calculating f1 score\n",
    "    return tf.py_func(f1_score(y_true, y_pred,average='mirco'),tf.double)\n",
    "\n",
    "def preprocess(syn): # preprocesses given text to predict outcome\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english')) #denotes that English stopwords will be removed\n",
    "    eng_words = set(nltk.corpus.words.words())\n",
    "    table = str.maketrans(\"\",\"\",string.punctuation) #table that will help to remove punctuation marks\n",
    "    word_tokens = word_tokenize(str(syn)) #tokenize description\n",
    "    filtered_sentence = [lemmatizer.lemmatize(w.lower()) for w in word_tokens if (not lemmatizer.lemmatize(w.lower()) in stop_words) and (w.isalpha()) and (lemmatizer.lemmatize(w.lower()) in eng_words)] #remove the stop words and save it to an array\n",
    "    syn = str(filtered_sentence).translate(table) #string is cleansed from punctuation marks after stop words are cleansed\n",
    "    syn = word_tokenize(str(syn))\n",
    "    return syn\n",
    "\n",
    "def predict_genre(synopsis, vect): # used for predicting genres\n",
    "    synopsis = preprocess(synopsis) #preprocess inputted test\n",
    "    encoded_syn = vect.texts_to_sequences([synopsis]) # make preprocessed text into a sequence\n",
    "    deneme = pad_text(encoded_syn[0], 100) \n",
    "    padded_syn = pad_sequences(encoded_syn, maxlen=100, padding='post') # pad text with maximum length of 100 (is a different function than the one in keras, keras padder doesn't work here, pads each word into a different array or pads letter by letter)\n",
    "    predictions2=model.predict([padded_syn]) #predict given input's genres\n",
    "    pred2 = predictions2.copy()\n",
    "    pred2[pred2>=0.3]=1 # if activation score is above the threshold, make in a 1, else a 0\n",
    "    pred2[pred2<0.3]=0\n",
    "    '''print(padded_syn)\n",
    "    print(len(pred2[0]))\n",
    "    print(len(pred2))\n",
    "    print(pred2)'''\n",
    "    print(\"Predicted genres: \")\n",
    "    final = [] # used for final output string\n",
    "    for i in range(len(pred2[0])): # for each prediction, if final score is 1 count that as a predicted genre and print it as an output\n",
    "        if pred2[0][i] == 1:\n",
    "            final.append(list(genres_array)[i])\n",
    "            #final.append(\", \")\n",
    "    print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading output-0-10000.json\n",
      "reading output-10000-15000.json\n",
      "reading output-15000-20000.json\n",
      "reading output-20000-30000.json\n",
      "reading output-30000-40000.json\n",
      "reading output-40000-50000.json\n",
      "reading output-50000-60000.json\n",
      "reading output-60000-70000.json\n",
      "reading output-70000-80000.json\n",
      "reading output-80000-90000.json\n",
      "reading output-90000-100000.json\n"
     ]
    }
   ],
   "source": [
    "vect = None\n",
    "\n",
    "    \n",
    "files = os.listdir(\"data\") # list all files in a spesific directory\n",
    "\n",
    "df = pd.DataFrame(columns = ['descs', 'genres']) # creates a empty dataframe to be later filled\n",
    "for file in files: # for each file in the directory\n",
    "    df = df.append(add_to_dataframe(file), ignore_index = True) # append file's contents to the dataframe with tags of descs and genres\n",
    "\n",
    "df['genres']=df['genres'].apply(remove_spaces) # makes genres into a string with no space\n",
    "\n",
    "genres_array = [] # stores all the genres in a set to later use them to label neural network outputs\n",
    "for x in df['genres']: # for each movie's genres\n",
    "        for y in x.split(\",\"): #split them to find all genres\n",
    "            if y not in genres_array:\n",
    "                genres_array.append(y) #add them to an array if they are not in the array. Since we are going to need indexes, we can't use sets for this step.\n",
    "\n",
    "train = df.iloc[:int(len(df)*0.85)] # divides data to two parts to use them for training and testing\n",
    "test = df.iloc[int(len(df)*0.85)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = genre_vectorize(train['genres']) #vectorize dataframes\n",
    "y_test=genre_vectorize(test['genres'])\n",
    "\n",
    "vect = Tokenizer()\n",
    "vect.fit_on_texts(train['descs'])\n",
    "vocab_size = len(vect.word_index) + 1 # total number of different words\n",
    "\n",
    "encoded_docs_train = vect.texts_to_sequences(train['descs']) #turns train tokens into sequences\n",
    "max_length = vocab_size\n",
    "padded_docs_train = pad_sequences(encoded_docs_train,maxlen=100, padding='post') # pads them to be maximum of 100 words length\n",
    "\n",
    "encoded_docs_test = vect.texts_to_sequences(test['descs']) #same process with test tokens\n",
    "padded_docs_test = pad_sequences(encoded_docs_test,maxlen=100, padding='post')\n",
    "\n",
    "threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 256)          6277888   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 96, 128)           163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 48, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               786560    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 7,230,867\n",
      "Trainable params: 7,230,867\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "1618/1618 [==============================] - 193s 119ms/step - loss: 0.3085 - val_loss: 0.2343\n",
      "Epoch 2/3\n",
      "1618/1618 [==============================] - 185s 114ms/step - loss: 0.2339 - val_loss: 0.2121\n",
      "Epoch 3/3\n",
      "1618/1618 [==============================] - 184s 113ms/step - loss: 0.1970 - val_loss: 0.2082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bb693a8f48>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential() # create a sequential neural network\n",
    "model.add(Embedding(vocab_size, output_dim=256, input_length=100)) \n",
    "model.add(Conv1D(128, kernel_size = 5, activation = 'relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(128, activation='sigmoid'))\n",
    "model.add(Dropout(.15))\n",
    "model.add(layers.Dense(19, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "model.fit(padded_docs_train, y_train, epochs=3, verbose=1, validation_data=(padded_docs_test, y_test), batch_size=32) # fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold:  0.3\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4918, Recall: 0.5904, F1-measure: 0.5366\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict([padded_docs_test]) # get a list of predictions for each data in test set\n",
    "\n",
    "#calculates success rates by making values 1 if above threshold, 0 otherwise\n",
    "print(\"For threshold: \", threshold)\n",
    "pred=predictions.copy()\n",
    "      \n",
    "pred[pred>=threshold]=1\n",
    "pred[pred<threshold]=0\n",
    "\n",
    "      \n",
    "precision = precision_score(y_test, pred, average='micro')\n",
    "recall = recall_score(y_test, pred, average='micro')\n",
    "f1 = f1_score(y_test, pred, average='micro')\n",
    "       \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter movie synopsis: \n",
      "A family heads to an isolated hotel for the winter sees horrific forebodings from both past and future.where a sinister presence influences the father into violence, while his psychic son \n",
      "Predicted genres: \n",
      "['Thriller', 'Horror']\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter movie synopsis: \") \n",
    "synopsis = input()\n",
    "predict_genre(synopsis, vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
